{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6845423d",
   "metadata": {},
   "source": [
    "# LiteMatting Model Training in Google Colab\n",
    "\n",
    "This notebook provides a complete training pipeline for the LiteMatting model using the Adobe Composition-1K dataset. The model is designed for high-quality image matting (background removal) with efficient mobile-friendly architecture.\n",
    "\n",
    "## Features:\n",
    "- ‚úÖ Mobile-optimized matting network\n",
    "- ‚úÖ Adobe Composition-1K dataset support\n",
    "- ‚úÖ GPU accelerated training\n",
    "- ‚úÖ Comprehensive loss functions (Alpha, Composition, Gradient, Laplacian)\n",
    "- ‚úÖ Real-time visualization\n",
    "- ‚úÖ Model checkpointing\n",
    "- ‚úÖ Evaluation metrics (SAD, MSE, Gradient Error)\n",
    "\n",
    "## Requirements:\n",
    "- Google Colab with GPU enabled\n",
    "- Adobe Composition-1K dataset (download link provided below)\n",
    "- Student account for academic dataset access\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f025f3f",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "First, let's install all the necessary dependencies for training the LiteMatting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install opencv-python-headless\n",
    "!pip install albumentations\n",
    "!pip install imgaug\n",
    "!pip install scikit-image\n",
    "!pip install tqdm\n",
    "!pip install tensorboard\n",
    "!pip install matplotlib\n",
    "!pip install pillow\n",
    "!pip install scipy\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available. Please enable GPU in Runtime -> Change runtime type -> Hardware accelerator -> GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44927f33",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare the Adobe Composition-1K Dataset\n",
    "\n",
    "The Adobe Composition-1K dataset is the standard benchmark for image matting. You'll need to download it manually due to academic licensing requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c42977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download instructions\n",
    "print(\"üìñ DATASET DOWNLOAD INSTRUCTIONS:\")\n",
    "print(\"1. Visit: https://sites.google.com/view/deepimagematting\")\n",
    "print(\"2. Request access to Adobe Composition-1K dataset with your student email\")\n",
    "print(\"3. Download and extract to Google Drive\")\n",
    "print(\"4. Upload to Colab or mount Google Drive\")\n",
    "print()\n",
    "print(\"Expected directory structure:\")\n",
    "print(\"Composition-1K/\")\n",
    "print(\"‚îú‚îÄ‚îÄ Training_set/\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ fg/           # Foreground images\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ alpha/        # Alpha mattes (ground truth)\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ bg/           # Background images\") \n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ merged/       # Composite images\")\n",
    "print(\"‚îÇ   ‚îî‚îÄ‚îÄ trimaps/      # Trimap annotations\")\n",
    "print(\"‚îî‚îÄ‚îÄ Test_set/\")\n",
    "print(\"    ‚îú‚îÄ‚îÄ fg/\")\n",
    "print(\"    ‚îú‚îÄ‚îÄ alpha/\")\n",
    "print(\"    ‚îú‚îÄ‚îÄ bg/\")\n",
    "print(\"    ‚îú‚îÄ‚îÄ merged/\")\n",
    "print(\"    ‚îî‚îÄ‚îÄ trimaps/\")\n",
    "print()\n",
    "\n",
    "# Mount Google Drive (if dataset is stored there)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set dataset path (update this to your dataset location)\n",
    "DATASET_PATH = \"/content/drive/MyDrive/Composition-1K\"  # Update this path!\n",
    "\n",
    "import os\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚úÖ Dataset found at: {DATASET_PATH}\")\n",
    "    \n",
    "    # Check dataset structure\n",
    "    train_path = os.path.join(DATASET_PATH, \"Training_set\")\n",
    "    test_path = os.path.join(DATASET_PATH, \"Test_set\")\n",
    "    \n",
    "    if os.path.exists(train_path):\n",
    "        print(f\"‚úÖ Training set found\")\n",
    "        for subfolder in ['fg', 'alpha', 'bg', 'merged', 'trimaps']:\n",
    "            subfolder_path = os.path.join(train_path, subfolder)\n",
    "            if os.path.exists(subfolder_path):\n",
    "                count = len([f for f in os.listdir(subfolder_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "                print(f\"   - {subfolder}: {count} files\")\n",
    "    \n",
    "    if os.path.exists(test_path):\n",
    "        print(f\"‚úÖ Test set found\")\n",
    "        for subfolder in ['fg', 'alpha', 'bg', 'merged', 'trimaps']:\n",
    "            subfolder_path = os.path.join(test_path, subfolder)\n",
    "            if os.path.exists(subfolder_path):\n",
    "                count = len([f for f in os.listdir(subfolder_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "                print(f\"   - {subfolder}: {count} files\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found at: {DATASET_PATH}\")\n",
    "    print(\"Please update DATASET_PATH variable to point to your dataset location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2bd7a",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Augmentation\n",
    "\n",
    "Implement PyTorch Dataset and DataLoader classes for efficient data loading with proper augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Composition1KDataset(Dataset):\n",
    "    def __init__(self, data_root, mode='train', transform=None, input_size=512):\n",
    "        \"\"\"\n",
    "        Adobe Composition-1K Dataset\n",
    "        \n",
    "        Args:\n",
    "            data_root: Path to Composition-1K dataset\n",
    "            mode: 'train' or 'test'\n",
    "            transform: Data augmentation transforms\n",
    "            input_size: Input image size for training\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.mode = mode\n",
    "        self.input_size = input_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.composite_path = os.path.join(data_root, 'Training_set', 'merged')\n",
    "            self.alpha_path = os.path.join(data_root, 'Training_set', 'alpha')\n",
    "            self.trimap_path = os.path.join(data_root, 'Training_set', 'trimaps')\n",
    "        else:\n",
    "            self.composite_path = os.path.join(data_root, 'Test_set', 'merged')\n",
    "            self.alpha_path = os.path.join(data_root, 'Test_set', 'alpha')\n",
    "            self.trimap_path = os.path.join(data_root, 'Test_set', 'trimaps')\n",
    "        \n",
    "        # Get list of images\n",
    "        self.image_list = []\n",
    "        if os.path.exists(self.composite_path):\n",
    "            self.image_list = sorted([f for f in os.listdir(self.composite_path) \n",
    "                                    if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        \n",
    "        print(f\"Found {len(self.image_list)} images in {mode} set\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def load_image(self, path):\n",
    "        \"\"\"Load image and convert to RGB\"\"\"\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not load image: {path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    \n",
    "    def load_alpha(self, path):\n",
    "        \"\"\"Load alpha matte\"\"\"\n",
    "        alpha = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if alpha is None:\n",
    "            raise ValueError(f\"Could not load alpha: {path}\")\n",
    "        return alpha.astype(np.float32) / 255.0\n",
    "    \n",
    "    def generate_trimap(self, alpha, k_size=10):\n",
    "        \"\"\"Generate trimap from alpha matte\"\"\"\n",
    "        alpha_uint8 = (alpha * 255).astype(np.uint8)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k_size, k_size))\n",
    "        \n",
    "        eroded = cv2.erode(alpha_uint8, kernel, iterations=1)\n",
    "        dilated = cv2.dilate(alpha_uint8, kernel, iterations=1)\n",
    "        \n",
    "        trimap = np.zeros_like(alpha_uint8)\n",
    "        trimap[eroded > 128] = 255  # Foreground\n",
    "        trimap[(dilated > 128) & (eroded <= 128)] = 128  # Unknown\n",
    "        return trimap\n",
    "    \n",
    "    def trimap_to_clicks(self, trimap):\n",
    "        \"\"\"Convert trimap to click guidance maps\"\"\"\n",
    "        h, w = trimap.shape\n",
    "        clicks = np.zeros((h, w, 2), dtype=np.float32)\n",
    "        \n",
    "        fg_mask = (trimap == 255).astype(np.uint8)\n",
    "        bg_mask = (trimap == 0).astype(np.uint8)\n",
    "        \n",
    "        if np.sum(fg_mask) > 0:\n",
    "            dt_fg = cv2.distanceTransform(1 - fg_mask, cv2.DIST_L2, 0)\n",
    "            clicks[:, :, 0] = np.exp(-dt_fg**2 / (2 * (0.05 * 320)**2))\n",
    "        \n",
    "        if np.sum(bg_mask) > 0:\n",
    "            dt_bg = cv2.distanceTransform(1 - bg_mask, cv2.DIST_L2, 0)\n",
    "            clicks[:, :, 1] = np.exp(-dt_bg**2 / (2 * (0.05 * 320)**2))\n",
    "        \n",
    "        return clicks\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "        base_name = os.path.splitext(image_name)[0]\n",
    "        \n",
    "        # Load composite image\n",
    "        composite_path = os.path.join(self.composite_path, image_name)\n",
    "        composite = self.load_image(composite_path)\n",
    "        \n",
    "        # Load alpha matte\n",
    "        alpha_path = os.path.join(self.alpha_path, image_name)\n",
    "        if not os.path.exists(alpha_path):\n",
    "            alpha_path = os.path.join(self.alpha_path, base_name + '.png')\n",
    "        alpha = self.load_alpha(alpha_path)\n",
    "        \n",
    "        # Load or generate trimap\n",
    "        trimap_file = os.path.join(self.trimap_path, base_name + '.png')\n",
    "        if os.path.exists(trimap_file):\n",
    "            trimap = cv2.imread(trimap_file, cv2.IMREAD_GRAYSCALE)\n",
    "        else:\n",
    "            trimap = self.generate_trimap(alpha)\n",
    "        \n",
    "        # Convert trimap to one-hot encoding\n",
    "        trimap_onehot = np.zeros((trimap.shape[0], trimap.shape[1], 3), dtype=np.float32)\n",
    "        trimap_onehot[:, :, 0] = (trimap == 0).astype(np.float32)    # Background\n",
    "        trimap_onehot[:, :, 1] = (trimap == 128).astype(np.float32)  # Unknown\n",
    "        trimap_onehot[:, :, 2] = (trimap == 255).astype(np.float32)  # Foreground\n",
    "        \n",
    "        # Generate click guidance\n",
    "        clicks = self.trimap_to_clicks(trimap)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=composite,\n",
    "                mask=alpha,\n",
    "                masks=[trimap_onehot[:,:,0], trimap_onehot[:,:,1], trimap_onehot[:,:,2], \n",
    "                       clicks[:,:,0], clicks[:,:,1]]\n",
    "            )\n",
    "            composite = transformed['image']\n",
    "            alpha = transformed['mask']\n",
    "            trimap_bg, trimap_unk, trimap_fg, click_fg, click_bg = transformed['masks']\n",
    "            \n",
    "            trimap_onehot = np.stack([trimap_bg, trimap_unk, trimap_fg], axis=2)\n",
    "            clicks = np.stack([click_fg, click_bg], axis=2)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        if not torch.is_tensor(composite):\n",
    "            composite = torch.from_numpy(composite.transpose(2, 0, 1)).float() / 255.0\n",
    "        if not torch.is_tensor(alpha):\n",
    "            alpha = torch.from_numpy(alpha).unsqueeze(0).float()\n",
    "        \n",
    "        trimap_tensor = torch.from_numpy(trimap_onehot.transpose(2, 0, 1)).float()\n",
    "        clicks_tensor = torch.from_numpy(clicks.transpose(2, 0, 1)).float()\n",
    "        \n",
    "        return {\n",
    "            'image': composite,\n",
    "            'alpha': alpha,\n",
    "            'trimap': trimap_tensor,\n",
    "            'clicks': clicks_tensor,\n",
    "            'name': image_name\n",
    "        }\n",
    "\n",
    "def get_train_transforms(input_size=512):\n",
    "    \"\"\"Get training data augmentation transforms\"\"\"\n",
    "    return A.Compose([\n",
    "        A.RandomResizedCrop(input_size, input_size, scale=(0.8, 1.0)),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.5),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
    "    ], additional_targets={\n",
    "        'mask': 'mask',\n",
    "        'masks': 'masks'\n",
    "    })\n",
    "\n",
    "def get_val_transforms(input_size=512):\n",
    "    \"\"\"Get validation transforms\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(input_size, input_size),\n",
    "    ], additional_targets={\n",
    "        'mask': 'mask',\n",
    "        'masks': 'masks'\n",
    "    })\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DataLoader\"\"\"\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    alphas = torch.stack([item['alpha'] for item in batch])\n",
    "    trimaps = torch.stack([item['trimap'] for item in batch])\n",
    "    clicks = torch.stack([item['clicks'] for item in batch])\n",
    "    names = [item['name'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'alpha': alphas,\n",
    "        'trimap': trimaps,\n",
    "        'clicks': clicks,\n",
    "        'name': names\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Dataset classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a272650",
   "metadata": {},
   "source": [
    "## 4. Define the MobileMatting Model\n",
    "\n",
    "Copy the complete MobileMatting model architecture with all required components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c34b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def GN(x):\n",
    "    if x % 32 == 0:\n",
    "        return nn.GroupNorm(32, x)\n",
    "    if x % 16 == 0:\n",
    "        return nn.GroupNorm(16, x)\n",
    "    if x % 8 == 0:\n",
    "        return nn.GroupNorm(8, x)\n",
    "    return nn.GroupNorm(4, x)\n",
    "\n",
    "class MSLPPM(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(MSLPPM, self).__init__()\n",
    "        mid_channels = int(in_channels / 2)\n",
    "        inter_channels = int(mid_channels / 4)\n",
    "        self.trans = nn.Sequential(nn.Conv2d(in_channels, mid_channels, 1, 1, 0, bias=False),\n",
    "                                   GN(mid_channels),\n",
    "                                   nn.ReLU(True),\n",
    "                                   nn.Conv2d(mid_channels, inter_channels, 1, 1, 0, bias=False),\n",
    "                                   GN(inter_channels),\n",
    "                                   nn.ReLU(True)\n",
    "                                   )\n",
    "        self.pool1 = nn.AdaptiveAvgPool2d((5, 5))\n",
    "        self.pool2 = nn.AdaptiveAvgPool2d((13, 13))\n",
    "        self.pool3 = nn.AdaptiveAvgPool2d((1, None))\n",
    "        self.pool4 = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool5 = nn.AdaptiveAvgPool2d((15, 7))\n",
    "        self.pool6 = nn.AdaptiveAvgPool2d((7, 15))\n",
    "        self.pool7 = nn.AdaptiveAvgPool2d((23, 11))\n",
    "        self.pool8 = nn.AdaptiveAvgPool2d((11, 23))\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (3, 3), 1, (1, 1), bias=False),\n",
    "                                   GN(inter_channels),\n",
    "                                   nn.ReLU(True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (3, 3), 1, (1, 1), bias=False),\n",
    "                                   GN(inter_channels),\n",
    "                                   nn.ReLU(True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (1, 3), 1, (0, 1), bias=False),\n",
    "                                   GN(inter_channels),\n",
    "                                   nn.ReLU(True))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (3, 1), 1, (1, 0), bias=False),\n",
    "                                   GN(inter_channels),\n",
    "                                   nn.ReLU(True))\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (5, 3), 1, (2, 1), bias=False),\n",
    "                                   GN(inter_channels),\n",
    "                                   nn.ReLU(True))\n",
    "        self.conv6 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (3, 5), 1, (1, 2), bias=False),\n",
    "                                   GN(inter_channels),\n",
    "                                   nn.ReLU(True))\n",
    "        self.conv7 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (7, 3), 1, (3, 1), bias=False),\n",
    "                                   GN(inter_channels),\n",
    "                                   nn.ReLU(True))\n",
    "        self.conv8 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (3, 7), 1, (1, 3), bias=False),\n",
    "                                   GN(inter_channels),\n",
    "                                   nn.ReLU(True))\n",
    "        self.conv = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),\n",
    "                                  GN(inter_channels),\n",
    "                                  nn.ReLU(True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        x = self.trans(x)\n",
    "        x1 = F.interpolate(self.conv1(self.pool1(x)), size=(h, w), mode='bilinear', align_corners=False)\n",
    "        x2 = F.interpolate(self.conv2(self.pool2(x)), size=(h, w), mode='bilinear', align_corners=False)\n",
    "        x3 = F.interpolate(self.conv3(self.pool3(x)), size=(h, w), mode='bilinear', align_corners=False)\n",
    "        x4 = F.interpolate(self.conv4(self.pool4(x)), size=(h, w), mode='bilinear', align_corners=False)\n",
    "        x5 = F.interpolate(self.conv5(self.pool5(x)), size=(h, w), mode='bilinear', align_corners=False)\n",
    "        x6 = F.interpolate(self.conv6(self.pool6(x)), size=(h, w), mode='bilinear', align_corners=False)\n",
    "        x7 = F.interpolate(self.conv7(self.pool7(x)), size=(h, w), mode='bilinear', align_corners=False)\n",
    "        x8 = F.interpolate(self.conv8(self.pool8(x)), size=(h, w), mode='bilinear', align_corners=False)\n",
    "        s = self.conv(F.relu_(x1 + x2))\n",
    "        l = self.conv(F.relu_(x3 + x4))\n",
    "        m = self.conv(F.relu_(x5 + x6))\n",
    "        n = self.conv(F.relu_(x7 + x8))\n",
    "        out = torch.cat([s, m, n, l], dim=1)\n",
    "        return out\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, dilation, expand_ratio, batch_norm):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        BatchNorm2d = batch_norm\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "        self.kernel_size = 3\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation, groups=hidden_dim, bias=False),\n",
    "                BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation, groups=hidden_dim, bias=False),\n",
    "                BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def fixed_padding(self, inputs, kernel_size, dilation):\n",
    "        kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "        pad_total = kernel_size_effective - 1\n",
    "        pad_beg = pad_total // 2\n",
    "        pad_end = pad_total - pad_beg\n",
    "        padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n",
    "        return padded_inputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_pad = self.fixed_padding(x, self.kernel_size, dilation=self.dilation)\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x_pad)\n",
    "        else:\n",
    "            return self.conv(x_pad)\n",
    "\n",
    "class InvertedResidualLeaky(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, dilation, expand_ratio, batch_norm):\n",
    "        super(InvertedResidualLeaky, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        BatchNorm2d = batch_norm\n",
    "        hidden_dim = round(oup * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "        self.kernel_size = 3\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation, groups=hidden_dim, bias=False),\n",
    "                BatchNorm2d(hidden_dim),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                BatchNorm2d(hidden_dim),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation, groups=hidden_dim, bias=False),\n",
    "                BatchNorm2d(hidden_dim),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def fixed_padding(self, inputs, kernel_size, dilation):\n",
    "        kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "        pad_total = kernel_size_effective - 1\n",
    "        pad_beg = pad_total // 2\n",
    "        pad_end = pad_total - pad_beg\n",
    "        padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n",
    "        return padded_inputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_pad = self.fixed_padding(x, self.kernel_size, dilation=self.dilation)\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x_pad)\n",
    "        else:\n",
    "            return self.conv(x_pad)\n",
    "\n",
    "class PSPModule(nn.Module):\n",
    "    def __init__(self, sizes=(1, 2, 3, 6)):\n",
    "        super(PSPModule, self).__init__()\n",
    "        self.stages = nn.ModuleList([self._make_stage(size) for size in sizes])\n",
    "\n",
    "    def _make_stage(self, size):\n",
    "        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n",
    "        return prior\n",
    "\n",
    "    def forward(self, feats):\n",
    "        n, c, _, _ = feats.size()\n",
    "        priors = [stage(feats).view(n, c, -1) for stage in self.stages]\n",
    "        center = torch.cat(priors, -1)\n",
    "        return center\n",
    "\n",
    "class GFNB(nn.Module):\n",
    "    def __init__(self, low_in_channels, high_in_channels, out_channels, key_channels, value_channels, dropout=0):\n",
    "        super(GFNB, self).__init__()\n",
    "        self.in_channels = low_in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.key_channels = key_channels\n",
    "        self.value_channels = value_channels\n",
    "\n",
    "        self.f_key = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels, kernel_size=1, stride=1, padding=0),\n",
    "            GN(self.key_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.f_query = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=high_in_channels, out_channels=self.key_channels, kernel_size=1, stride=1, padding=0),\n",
    "            GN(self.key_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.f_value = nn.Conv2d(in_channels=self.in_channels, out_channels=self.value_channels, kernel_size=1,\n",
    "                                 stride=1, padding=0)\n",
    "        self.W = nn.Conv2d(in_channels=self.value_channels, out_channels=self.out_channels, kernel_size=1, stride=1,\n",
    "                           padding=0)\n",
    "        nn.init.constant_(self.W.weight, 0)\n",
    "        nn.init.constant_(self.W.bias, 0)\n",
    "        self.psp = PSPModule(sizes=(1, 3, 6, 8))\n",
    "\n",
    "        self.conv_bn_dropout = nn.Sequential(\n",
    "            nn.Conv2d(out_channels + high_in_channels, out_channels, kernel_size=1, padding=0),\n",
    "            GN(out_channels),\n",
    "            nn.Dropout2d(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, low_feats, high_feats):\n",
    "        batch_size, h, w = high_feats.size(0), high_feats.size(2), high_feats.size(3)\n",
    "        value = self.f_value(low_feats)\n",
    "        value = self.psp(value)\n",
    "        value = value.permute(0, 2, 1)\n",
    "\n",
    "        key = self.f_key(low_feats)\n",
    "        key = self.psp(key)\n",
    "\n",
    "        query = self.f_query(high_feats).view(batch_size, self.key_channels, -1)\n",
    "        query = query.permute(0, 2, 1)\n",
    "\n",
    "        sim_map = torch.matmul(query, key)\n",
    "        sim_map = (self.key_channels ** -.5) * sim_map\n",
    "        sim_map = F.softmax(sim_map, dim=-1)\n",
    "\n",
    "        context = torch.matmul(sim_map, value)\n",
    "        context = context.permute(0, 2, 1).contiguous()\n",
    "        context = context.view(batch_size, self.value_channels, *high_feats.size()[2:])\n",
    "        context = self.W(context)\n",
    "        output = self.conv_bn_dropout(torch.cat([context, high_feats], 1))\n",
    "        return output\n",
    "\n",
    "def conv_bn(inp, oup, k=3, s=1, BatchNorm2d=nn.BatchNorm2d):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, k, s, padding=k // 2, bias=False),\n",
    "        BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "class MobileMatting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileMatting, self).__init__()\n",
    "        output_stride = 32\n",
    "        BatchNorm2d = GN\n",
    "        width_mult = 1.\n",
    "        self.width_mult = width_mult\n",
    "        block = InvertedResidual\n",
    "        blockleaky = InvertedResidualLeaky\n",
    "\n",
    "        initial_channel = 32\n",
    "        current_stride = 1\n",
    "        rate = 1\n",
    "\n",
    "        inverted_residual_setting = [\n",
    "            [1, 32, 32, 1, 1, 1],\n",
    "            [6, 32, 24, 2, 2, 1],\n",
    "            [6, 24, 32, 3, 2, 1],\n",
    "            [6, 32, 64, 4, 2, 1],\n",
    "            [6, 64, 96, 3, 1, 1],\n",
    "            [6, 96, 160, 3, 2, 1],\n",
    "            [6, 240, 320, 1, 1, 1], ]\n",
    "\n",
    "        initial_channel = int(initial_channel * width_mult)\n",
    "        self.layerx = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            GN(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            GN(64),\n",
    "            self._build_layer(block, [4, 64, 64, 4, 1, 1], BatchNorm2d)\n",
    "        )\n",
    "        self.layer0 = conv_bn(3 + 3 + 2, initial_channel, 3, 2, BatchNorm2d)\n",
    "\n",
    "        current_stride *= 2\n",
    "\n",
    "        for i, setting in enumerate(inverted_residual_setting):\n",
    "            s = setting[4]\n",
    "            if current_stride == output_stride:\n",
    "                inverted_residual_setting[i][4] = 1\n",
    "                rate *= s\n",
    "                inverted_residual_setting[i][5] = rate\n",
    "            else:\n",
    "                current_stride *= s\n",
    "\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(32, 32, 3, 1, 1), BatchNorm2d(32), nn.ReLU(inplace=True))\n",
    "        self.layer2 = self._build_layer(block, inverted_residual_setting[1], BatchNorm2d, downsample=True)\n",
    "        self.layer3 = self._build_layer(block, inverted_residual_setting[2], BatchNorm2d, downsample=True)\n",
    "        self.layer4 = self._build_layer(block, inverted_residual_setting[3], BatchNorm2d, downsample=True)\n",
    "        self.layer5 = self._build_layer(block, inverted_residual_setting[4], BatchNorm2d)\n",
    "        self.layer6 = self._build_layer(block, inverted_residual_setting[5], BatchNorm2d, downsample=True)\n",
    "        self.layer7 = self._build_layer(block, inverted_residual_setting[6], BatchNorm2d)\n",
    "        self.ppm = MSLPPM(320)\n",
    "        self.gfnb = GFNB(64, 160, 80, 80, 80)\n",
    "        self.dfpool = nn.Sequential(nn.Conv2d(320 + 160, 128, 1, 1, 0), nn.GroupNorm(16, 128), nn.LeakyReLU(True))\n",
    "        self.uper1 = self._build_layer(blockleaky, [4, 160 + 128, 160, 3, 1, 1], BatchNorm2d)\n",
    "        self.uper2 = self._build_layer(blockleaky, [4, 256, 128, 3, 1, 1], BatchNorm2d)\n",
    "        self.uper3 = self._build_layer(blockleaky, [4, 160, 96, 3, 1, 1], BatchNorm2d)\n",
    "        self.uper4 = self._build_layer(blockleaky, [4, 120, 64, 3, 1, 1], BatchNorm2d)\n",
    "        self.uper5 = nn.Sequential(nn.Conv2d(96, 48, 3, 1, 1), BatchNorm2d(48), nn.PReLU(48),\n",
    "                                   nn.Conv2d(48, 32, 3, 1, 1), BatchNorm2d(32), nn.PReLU(32))\n",
    "        self.out = nn.Sequential(nn.Conv2d(32 + 6, 24, 3, 1, 1), nn.PReLU(24), nn.Conv2d(24, 12, 3, 1, 1), nn.PReLU(12),\n",
    "                                 nn.Conv2d(12, 1, 3, 1, 1))\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def _build_layer(self, block, layer_setting, batch_norm, downsample=False):\n",
    "        t, p, c, n, s, d = layer_setting\n",
    "        input_channel = int(p * self.width_mult)\n",
    "        output_channel = int(c * self.width_mult)\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            if i == 0:\n",
    "                d0 = d\n",
    "                if downsample:\n",
    "                    d0 = d // 2 if d > 1 else 1\n",
    "                layers.append(block(input_channel, output_channel, s, d0, expand_ratio=t, batch_norm=batch_norm))\n",
    "            else:\n",
    "                layers.append(block(input_channel, output_channel, 1, d, expand_ratio=t, batch_norm=batch_norm))\n",
    "            input_channel = output_channel\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img, tri, sixc):\n",
    "        input = torch.cat((img * 2. - 1., tri, sixc), dim=1)\n",
    "\n",
    "        l0 = self.layer0(input)\n",
    "        l1 = self.layer1(l0)\n",
    "        l2 = self.layer2(l1)\n",
    "        l3 = self.layer3(l2)\n",
    "        l4 = self.layer4(l3)\n",
    "        l5 = self.layer5(l4)\n",
    "        l6 = self.layer6(l5)\n",
    "\n",
    "        _, _, h, w = l6.shape\n",
    "        unkown = torch.nn.functional.interpolate(tri[:, 1:2] + tri[:, 2:3], size=(h, w), mode='nearest')\n",
    "        lx = self.layerx(img)\n",
    "        l6_g = self.gfnb(lx, l6 * unkown)\n",
    "        l6_c = torch.cat((l6, l6_g), dim=1)\n",
    "\n",
    "        l7 = self.layer7(l6_c)\n",
    "        feats = self.ppm(l7)\n",
    "        l7 = torch.cat([l7, feats], 1)\n",
    "        l7 = self.dfpool(l7)\n",
    "\n",
    "        lmid = torch.cat((l6, l7), dim=1)\n",
    "        lmid = self.uper1(lmid)\n",
    "        lmid = torch.cat((self.up(lmid), l5), dim=1)\n",
    "        lmid = self.uper2(lmid)\n",
    "        lmid = torch.cat((self.up(lmid), l3), dim=1)\n",
    "        lmid = self.uper3(lmid)\n",
    "        lmid = torch.cat((self.up(lmid), l2), dim=1)\n",
    "        lmid = self.uper4(lmid)\n",
    "        lmid = torch.cat((self.up(lmid), l1), dim=1)\n",
    "        lmid = self.uper5(lmid)\n",
    "        lmid = self.up(lmid)\n",
    "        lmid = torch.cat((lmid, img, tri), dim=1)\n",
    "        lmid = self.out(lmid)\n",
    "        lmid = torch.clamp(lmid, 0, 1)\n",
    "        return lmid\n",
    "\n",
    "# Test model creation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MobileMatting().to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"‚úÖ MobileMatting model created successfully!\")\n",
    "print(f\"üìä Total parameters: {total_params:,}\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"üíæ Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f66a747",
   "metadata": {},
   "source": [
    "## 5. Define Loss Functions and Metrics\n",
    "\n",
    "Implement comprehensive loss functions for matting including alpha loss, composition loss, gradient loss, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621da5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_loss(pred_alpha, gt_alpha, trimap):\n",
    "    \"\"\"Alpha prediction loss (L1 loss in unknown regions)\"\"\"\n",
    "    unknown_mask = (trimap[:, 1:2, :, :] > 0.5).float()\n",
    "    diff = torch.abs(pred_alpha - gt_alpha)\n",
    "    loss = torch.sum(diff * unknown_mask) / (torch.sum(unknown_mask) + 1e-8)\n",
    "    return loss\n",
    "\n",
    "def composition_loss(pred_alpha, gt_alpha, image, trimap):\n",
    "    \"\"\"Composition loss - compares composited images\"\"\"\n",
    "    unknown_mask = (trimap[:, 1:2, :, :] > 0.5).float()\n",
    "    pred_comp = pred_alpha * image\n",
    "    gt_comp = gt_alpha * image\n",
    "    diff = torch.abs(pred_comp - gt_comp)\n",
    "    loss = torch.sum(diff * unknown_mask) / (torch.sum(unknown_mask) + 1e-8)\n",
    "    return loss\n",
    "\n",
    "def gradient_loss(pred_alpha, gt_alpha, trimap):\n",
    "    \"\"\"Gradient loss to preserve fine details\"\"\"\n",
    "    unknown_mask = (trimap[:, 1:2, :, :] > 0.5).float()\n",
    "    \n",
    "    # Sobel filters for gradient computation\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "                          dtype=torch.float32, device=pred_alpha.device).view(1, 1, 3, 3)\n",
    "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n",
    "                          dtype=torch.float32, device=pred_alpha.device).view(1, 1, 3, 3)\n",
    "    \n",
    "    # Compute gradients\n",
    "    pred_grad_x = F.conv2d(pred_alpha, sobel_x, padding=1)\n",
    "    pred_grad_y = F.conv2d(pred_alpha, sobel_y, padding=1)\n",
    "    gt_grad_x = F.conv2d(gt_alpha, sobel_x, padding=1)\n",
    "    gt_grad_y = F.conv2d(gt_alpha, sobel_y, padding=1)\n",
    "    \n",
    "    # Gradient magnitude\n",
    "    pred_grad = torch.sqrt(pred_grad_x**2 + pred_grad_y**2 + 1e-8)\n",
    "    gt_grad = torch.sqrt(gt_grad_x**2 + gt_grad_y**2 + 1e-8)\n",
    "    \n",
    "    # L1 loss on gradients in unknown regions\n",
    "    diff = torch.abs(pred_grad - gt_grad)\n",
    "    loss = torch.sum(diff * unknown_mask) / (torch.sum(unknown_mask) + 1e-8)\n",
    "    return loss\n",
    "\n",
    "def laplacian_loss(pred_alpha, gt_alpha, trimap):\n",
    "    \"\"\"Laplacian loss for smoothness\"\"\"\n",
    "    unknown_mask = (trimap[:, 1:2, :, :] > 0.5).float()\n",
    "    \n",
    "    # Laplacian kernel\n",
    "    laplacian_kernel = torch.tensor([[0, -1, 0], [-1, 4, -1], [0, -1, 0]], \n",
    "                                   dtype=torch.float32, device=pred_alpha.device).view(1, 1, 3, 3)\n",
    "    \n",
    "    # Apply Laplacian\n",
    "    pred_lap = F.conv2d(pred_alpha, laplacian_kernel, padding=1)\n",
    "    gt_lap = F.conv2d(gt_alpha, laplacian_kernel, padding=1)\n",
    "    \n",
    "    # L1 loss on Laplacian in unknown regions\n",
    "    diff = torch.abs(pred_lap - gt_lap)\n",
    "    loss = torch.sum(diff * unknown_mask) / (torch.sum(unknown_mask) + 1e-8)\n",
    "    return loss\n",
    "\n",
    "class MattingLoss(nn.Module):\n",
    "    \"\"\"Combined matting loss function\"\"\"\n",
    "    def __init__(self, alpha_weight=1.0, comp_weight=1.0, grad_weight=1.0, lap_weight=1.0):\n",
    "        super(MattingLoss, self).__init__()\n",
    "        self.alpha_weight = alpha_weight\n",
    "        self.comp_weight = comp_weight\n",
    "        self.grad_weight = grad_weight\n",
    "        self.lap_weight = lap_weight\n",
    "    \n",
    "    def forward(self, pred_alpha, gt_alpha, image, trimap):\n",
    "        losses = {}\n",
    "        \n",
    "        # Alpha prediction loss\n",
    "        losses['alpha'] = alpha_loss(pred_alpha, gt_alpha, trimap)\n",
    "        \n",
    "        # Composition loss\n",
    "        losses['composition'] = composition_loss(pred_alpha, gt_alpha, image, trimap)\n",
    "        \n",
    "        # Gradient loss\n",
    "        losses['gradient'] = gradient_loss(pred_alpha, gt_alpha, trimap)\n",
    "        \n",
    "        # Laplacian loss\n",
    "        losses['laplacian'] = laplacian_loss(pred_alpha, gt_alpha, trimap)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.alpha_weight * losses['alpha'] + \n",
    "                     self.comp_weight * losses['composition'] + \n",
    "                     self.grad_weight * losses['gradient'] + \n",
    "                     self.lap_weight * losses['laplacian'])\n",
    "        \n",
    "        losses['total'] = total_loss\n",
    "        return losses\n",
    "\n",
    "# Evaluation metrics\n",
    "def compute_sad(pred_alpha, gt_alpha, trimap):\n",
    "    \"\"\"Sum of Absolute Differences\"\"\"\n",
    "    unknown_mask = (trimap[:, 1:2, :, :] > 0.5).float()\n",
    "    diff = torch.abs(pred_alpha - gt_alpha) * unknown_mask\n",
    "    sad = torch.sum(diff, dim=[1, 2, 3]) / 1000.0  # Scale to thousands\n",
    "    return sad.mean()\n",
    "\n",
    "def compute_mse(pred_alpha, gt_alpha, trimap):\n",
    "    \"\"\"Mean Squared Error in unknown regions\"\"\"\n",
    "    unknown_mask = (trimap[:, 1:2, :, :] > 0.5).float()\n",
    "    diff = (pred_alpha - gt_alpha) ** 2 * unknown_mask\n",
    "    mse = torch.sum(diff, dim=[1, 2, 3]) / torch.sum(unknown_mask, dim=[1, 2, 3])\n",
    "    return mse.mean()\n",
    "\n",
    "def compute_gradient_error(pred_alpha, gt_alpha, trimap):\n",
    "    \"\"\"Gradient error in unknown regions\"\"\"\n",
    "    unknown_mask = (trimap[:, 1:2, :, :] > 0.5).float()\n",
    "    \n",
    "    # Sobel filters\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "                          dtype=torch.float32, device=pred_alpha.device).view(1, 1, 3, 3)\n",
    "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n",
    "                          dtype=torch.float32, device=pred_alpha.device).view(1, 1, 3, 3)\n",
    "    \n",
    "    # Compute gradients\n",
    "    pred_grad_x = F.conv2d(pred_alpha, sobel_x, padding=1)\n",
    "    pred_grad_y = F.conv2d(pred_alpha, sobel_y, padding=1)\n",
    "    gt_grad_x = F.conv2d(gt_alpha, sobel_x, padding=1)\n",
    "    gt_grad_y = F.conv2d(gt_alpha, sobel_y, padding=1)\n",
    "    \n",
    "    # Gradient error\n",
    "    grad_error = torch.sqrt((pred_grad_x - gt_grad_x)**2 + (pred_grad_y - gt_grad_y)**2 + 1e-8)\n",
    "    grad_error = grad_error * unknown_mask\n",
    "    \n",
    "    error = torch.sum(grad_error, dim=[1, 2, 3]) / (torch.sum(unknown_mask, dim=[1, 2, 3]) + 1e-8)\n",
    "    return error.mean() / 1000.0  # Scale to thousands\n",
    "\n",
    "print(\"‚úÖ Loss functions and metrics defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b93eaa",
   "metadata": {},
   "source": [
    "## 6. Set Up Training Configuration and Data Loaders\n",
    "\n",
    "Configure training hyperparameters and create data loaders for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "class Config:\n",
    "    # Data\n",
    "    input_size = 512\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 4  # Reduced for Colab memory limits\n",
    "    val_batch_size = 2\n",
    "    epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "    num_workers = 2\n",
    "    \n",
    "    # Loss weights\n",
    "    alpha_weight = 1.0\n",
    "    comp_weight = 1.0\n",
    "    grad_weight = 1.0\n",
    "    lap_weight = 1.0\n",
    "    \n",
    "    # Logging and saving\n",
    "    log_interval = 50\n",
    "    save_interval = 5\n",
    "    checkpoint_dir = \"/content/checkpoints\"\n",
    "    log_dir = \"/content/logs\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(config.log_dir, exist_ok=True)\n",
    "\n",
    "# Create datasets (only if dataset exists)\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    # Create transforms\n",
    "    train_transform = get_train_transforms(config.input_size)\n",
    "    val_transform = get_val_transforms(config.input_size)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Composition1KDataset(\n",
    "        DATASET_PATH,\n",
    "        mode='train',\n",
    "        transform=train_transform,\n",
    "        input_size=config.input_size\n",
    "    )\n",
    "    \n",
    "    val_dataset = Composition1KDataset(\n",
    "        DATASET_PATH,\n",
    "        mode='test',\n",
    "        transform=val_transform,\n",
    "        input_size=config.input_size\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.val_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created successfully!\")\n",
    "    print(f\"üìä Training samples: {len(train_dataset)}\")\n",
    "    print(f\"üìä Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"üìä Training batches: {len(train_loader)}\")\n",
    "    print(f\"üìä Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Test data loading\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"\\nüîç Sample batch shapes:\")\n",
    "    print(f\"   Image: {sample_batch['image'].shape}\")\n",
    "    print(f\"   Alpha: {sample_batch['alpha'].shape}\")\n",
    "    print(f\"   Trimap: {sample_batch['trimap'].shape}\")\n",
    "    print(f\"   Clicks: {sample_batch['clicks'].shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dataset not found. Please update DATASET_PATH and re-run this cell.\")\n",
    "    train_loader = None\n",
    "    val_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42efad25",
   "metadata": {},
   "source": [
    "## 7. Training Loop Implementation\n",
    "\n",
    "Implement the main training loop with optimizer, scheduler, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MobileMatting().to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = MattingLoss(\n",
    "    alpha_weight=config.alpha_weight,\n",
    "    comp_weight=config.comp_weight,\n",
    "    grad_weight=config.grad_weight,\n",
    "    lap_weight=config.lap_weight\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.epochs,\n",
    "    eta_min=config.learning_rate * 0.1\n",
    ")\n",
    "\n",
    "# Tensorboard writer\n",
    "writer = SummaryWriter(config.log_dir)\n",
    "\n",
    "# Training state\n",
    "best_loss = float('inf')\n",
    "global_step = 0\n",
    "\n",
    "print(\"‚úÖ Training setup completed!\")\n",
    "print(f\"üéØ Device: {device}\")\n",
    "print(f\"üìà Total epochs: {config.epochs}\")\n",
    "print(f\"üìö Batch size: {config.batch_size}\")\n",
    "print(f\"üß† Learning rate: {config.learning_rate}\")\n",
    "\n",
    "# Memory optimization for Colab\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf0bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    epoch_losses = {\n",
    "        'total': 0.0,\n",
    "        'alpha': 0.0,\n",
    "        'composition': 0.0,\n",
    "        'gradient': 0.0,\n",
    "        'laplacian': 0.0\n",
    "    }\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        global global_step\n",
    "        \n",
    "        # Move to device\n",
    "        image = batch['image'].to(device)\n",
    "        alpha_gt = batch['alpha'].to(device)\n",
    "        trimap = batch['trimap'].to(device)\n",
    "        clicks = batch['clicks'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        alpha_pred = model(image, trimap, clicks)\n",
    "        \n",
    "        # Compute loss\n",
    "        losses = criterion(alpha_pred, alpha_gt, image, trimap)\n",
    "        \n",
    "        # Backward pass\n",
    "        losses['total'].backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        for key in epoch_losses:\n",
    "            epoch_losses[key] += losses[key].item()\n",
    "        \n",
    "        # Log to tensorboard\n",
    "        if global_step % config.log_interval == 0:\n",
    "            for key, value in losses.items():\n",
    "                writer.add_scalar(f'Train/{key}_loss', value.item(), global_step)\n",
    "            writer.add_scalar('Train/lr', optimizer.param_groups[0]['lr'], global_step)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f\"{losses['total'].item():.4f}\",\n",
    "            'Alpha': f\"{losses['alpha'].item():.4f}\",\n",
    "            'LR': f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        })\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "        # Memory cleanup for Colab\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Average losses\n",
    "    num_batches = len(train_loader)\n",
    "    for key in epoch_losses:\n",
    "        epoch_losses[key] /= num_batches\n",
    "    \n",
    "    return epoch_losses\n",
    "\n",
    "def validate_epoch(epoch):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    val_losses = {\n",
    "        'total': 0.0,\n",
    "        'alpha': 0.0,\n",
    "        'composition': 0.0,\n",
    "        'gradient': 0.0,\n",
    "        'laplacian': 0.0\n",
    "    }\n",
    "    val_metrics = {\n",
    "        'sad': 0.0,\n",
    "        'mse': 0.0,\n",
    "        'grad_error': 0.0\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=\"Validation\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move to device\n",
    "            image = batch['image'].to(device)\n",
    "            alpha_gt = batch['alpha'].to(device)\n",
    "            trimap = batch['trimap'].to(device)\n",
    "            clicks = batch['clicks'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            alpha_pred = model(image, trimap, clicks)\n",
    "            \n",
    "            # Compute loss\n",
    "            losses = criterion(alpha_pred, alpha_gt, image, trimap)\n",
    "            \n",
    "            # Compute metrics\n",
    "            sad = compute_sad(alpha_pred, alpha_gt, trimap)\n",
    "            mse = compute_mse(alpha_pred, alpha_gt, trimap)\n",
    "            grad_error = compute_gradient_error(alpha_pred, alpha_gt, trimap)\n",
    "            \n",
    "            # Update metrics\n",
    "            for key in val_losses:\n",
    "                val_losses[key] += losses[key].item()\n",
    "            \n",
    "            val_metrics['sad'] += sad.item()\n",
    "            val_metrics['mse'] += mse.item()\n",
    "            val_metrics['grad_error'] += grad_error.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f\"{losses['total'].item():.4f}\",\n",
    "                'SAD': f\"{sad.item():.2f}\",\n",
    "                'MSE': f\"{mse.item():.4f}\"\n",
    "            })\n",
    "    \n",
    "    # Average metrics\n",
    "    num_batches = len(val_loader)\n",
    "    for key in val_losses:\n",
    "        val_losses[key] /= num_batches\n",
    "    for key in val_metrics:\n",
    "        val_metrics[key] /= num_batches\n",
    "    \n",
    "    return val_losses, val_metrics\n",
    "\n",
    "def save_checkpoint(epoch, is_best=False):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'global_step': global_step,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    # Save latest checkpoint\n",
    "    checkpoint_path = os.path.join(config.checkpoint_dir, 'latest_checkpoint.pth')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    # Save best checkpoint\n",
    "    if is_best:\n",
    "        best_path = os.path.join(config.checkpoint_dir, 'best_checkpoint.pth')\n",
    "        torch.save(checkpoint, best_path)\n",
    "        print(f\"üíæ New best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "print(\"‚úÖ Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf065d",
   "metadata": {},
   "source": [
    "## 8. Start Training\n",
    "\n",
    "Execute the main training loop. This will take several hours depending on your dataset size and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8280f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run training if data loaders are available\n",
    "if train_loader is not None and val_loader is not None:\n",
    "    print(\"üöÄ Starting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Train\n",
    "        train_losses = train_epoch(epoch)\n",
    "        \n",
    "        # Validate\n",
    "        val_losses, val_metrics = validate_epoch(epoch)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log epoch results\n",
    "        print(f\"\\nüìä Epoch {epoch+1} Results:\")\n",
    "        print(f\"   Train Loss: {train_losses['total']:.4f}\")\n",
    "        print(f\"   Val Loss: {val_losses['total']:.4f}\")\n",
    "        print(f\"   Val SAD: {val_metrics['sad']:.2f}\")\n",
    "        print(f\"   Val MSE: {val_metrics['mse']:.4f}\")\n",
    "        print(f\"   Val Grad Error: {val_metrics['grad_error']:.4f}\")\n",
    "        print(f\"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Log to tensorboard\n",
    "        for key, value in val_losses.items():\n",
    "            writer.add_scalar(f'Val/{key}_loss', value, epoch)\n",
    "        for key, value in val_metrics.items():\n",
    "            writer.add_scalar(f'Val/{key}', value, epoch)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        is_best = val_losses['total'] < best_loss\n",
    "        if is_best:\n",
    "            best_loss = val_losses['total']\n",
    "        \n",
    "        if (epoch + 1) % config.save_interval == 0 or is_best:\n",
    "            save_checkpoint(epoch, is_best)\n",
    "        \n",
    "        # Early stopping check (optional)\n",
    "        # You can add early stopping logic here\n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nüéâ Training completed!\")\n",
    "    print(f\"‚è±Ô∏è  Total time: {total_time/3600:.2f} hours\")\n",
    "    print(f\"üèÜ Best validation loss: {best_loss:.4f}\")\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot start training - dataset not loaded.\")\n",
    "    print(\"Please ensure the dataset path is correct and re-run the data loading cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d4382",
   "metadata": {},
   "source": [
    "## 9. Model Inference and Visualization\n",
    "\n",
    "Test the trained model and visualize results on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92038c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path):\n",
    "    \"\"\"Load model checkpoint\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
    "        return False\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"‚úÖ Model loaded from checkpoint: {checkpoint_path}\")\n",
    "    print(f\"üìä Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"üìä Best loss: {checkpoint['best_loss']:.4f}\")\n",
    "    return True\n",
    "\n",
    "def visualize_predictions(num_samples=4):\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    if val_loader is None:\n",
    "        print(\"‚ùå Validation loader not available\")\n",
    "        return\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of validation data\n",
    "    sample_batch = next(iter(val_loader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Move to device\n",
    "        images = sample_batch['image'][:num_samples].to(device)\n",
    "        alphas_gt = sample_batch['alpha'][:num_samples].to(device)\n",
    "        trimaps = sample_batch['trimap'][:num_samples].to(device)\n",
    "        clicks = sample_batch['clicks'][:num_samples].to(device)\n",
    "        names = sample_batch['name'][:num_samples]\n",
    "        \n",
    "        # Predict\n",
    "        alphas_pred = model(images, trimaps, clicks)\n",
    "        \n",
    "        # Move back to CPU for visualization\n",
    "        images = images.cpu()\n",
    "        alphas_gt = alphas_gt.cpu()\n",
    "        alphas_pred = alphas_pred.cpu()\n",
    "        trimaps = trimaps.cpu()\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4*num_samples))\n",
    "        if num_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Input image\n",
    "            img = images[i].permute(1, 2, 0).numpy()\n",
    "            axes[i, 0].imshow(img)\n",
    "            axes[i, 0].set_title(f'Input Image\\\\n{names[i]}')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Trimap (unknown region)\n",
    "            trimap_vis = trimaps[i, 1].numpy()  # Unknown region\n",
    "            axes[i, 1].imshow(trimap_vis, cmap='gray')\n",
    "            axes[i, 1].set_title('Trimap (Unknown)')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Ground truth alpha\n",
    "            alpha_gt = alphas_gt[i, 0].numpy()\n",
    "            axes[i, 2].imshow(alpha_gt, cmap='gray')\n",
    "            axes[i, 2].set_title('Ground Truth Alpha')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            # Predicted alpha\n",
    "            alpha_pred = alphas_pred[i, 0].numpy()\n",
    "            axes[i, 3].imshow(alpha_pred, cmap='gray')\n",
    "            axes[i, 3].set_title('Predicted Alpha')\n",
    "            axes[i, 3].axis('off')\n",
    "            \n",
    "            # Composited result (with new background)\n",
    "            # Create a simple colored background for demo\n",
    "            bg_color = np.array([0.2, 0.8, 0.2])  # Green background\n",
    "            composite = alpha_pred[..., np.newaxis] * img + (1 - alpha_pred[..., np.newaxis]) * bg_color\n",
    "            composite = np.clip(composite, 0, 1)\n",
    "            axes[i, 4].imshow(composite)\n",
    "            axes[i, 4].set_title('Composite Result')\n",
    "            axes[i, 4].axis('off')\n",
    "            \n",
    "            # Calculate metrics for this sample\n",
    "            unknown_mask = trimap_vis > 0.5\n",
    "            if np.sum(unknown_mask) > 0:\n",
    "                sad = np.sum(np.abs(alpha_pred - alpha_gt) * unknown_mask) / 1000.0\n",
    "                mse = np.mean((alpha_pred - alpha_gt)**2 * unknown_mask)\n",
    "                print(f\"Sample {i+1} - SAD: {sad:.2f}, MSE: {mse:.4f}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def infer_on_custom_image(image_path, trimap_path=None):\n",
    "    \"\"\"Run inference on a custom image\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"‚ùå Could not load image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Resize to model input size\n",
    "    image_resized = cv2.resize(image, (config.input_size, config.input_size))\n",
    "    image_tensor = torch.from_numpy(image_resized.transpose(2, 0, 1)).float() / 255.0\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Create trimap (you can load from file or create automatically)\n",
    "    if trimap_path and os.path.exists(trimap_path):\n",
    "        trimap = cv2.imread(trimap_path, cv2.IMREAD_GRAYSCALE)\n",
    "        trimap = cv2.resize(trimap, (config.input_size, config.input_size))\n",
    "    else:\n",
    "        # Simple automatic trimap generation (for demo)\n",
    "        print(\"üîÑ Generating automatic trimap...\")\n",
    "        # This is a very basic trimap - in practice you'd want better trimap generation\n",
    "        trimap = np.ones((config.input_size, config.input_size), dtype=np.uint8) * 128  # All unknown\n",
    "        # Create some foreground and background regions\n",
    "        center = config.input_size // 2\n",
    "        trimap[:50, :] = 0  # Top background\n",
    "        trimap[-50:, :] = 0  # Bottom background\n",
    "        trimap[:, :50] = 0  # Left background\n",
    "        trimap[:, -50:] = 0  # Right background\n",
    "        trimap[center-100:center+100, center-100:center+100] = 255  # Center foreground\n",
    "    \n",
    "    # Convert trimap to one-hot\n",
    "    trimap_onehot = np.zeros((3, config.input_size, config.input_size), dtype=np.float32)\n",
    "    trimap_onehot[0] = (trimap == 0).astype(np.float32)    # Background\n",
    "    trimap_onehot[1] = (trimap == 128).astype(np.float32)  # Unknown\n",
    "    trimap_onehot[2] = (trimap == 255).astype(np.float32)  # Foreground\n",
    "    trimap_tensor = torch.from_numpy(trimap_onehot).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate click guidance (simplified)\n",
    "    clicks = np.zeros((2, config.input_size, config.input_size), dtype=np.float32)\n",
    "    clicks_tensor = torch.from_numpy(clicks).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        alpha_pred = model(image_tensor, trimap_tensor, clicks_tensor)\n",
    "    \n",
    "    # Resize alpha back to original size\n",
    "    alpha_pred = alpha_pred.squeeze().cpu().numpy()\n",
    "    alpha_pred = cv2.resize(alpha_pred, (w, h))\n",
    "    \n",
    "    # Visualize result\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.resize(trimap, (w, h)), cmap='gray')\n",
    "    axes[1].set_title('Trimap')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(alpha_pred, cmap='gray')\n",
    "    axes[2].set_title('Predicted Alpha')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Composite with new background\n",
    "    bg_color = np.array([0.2, 0.2, 0.8])  # Blue background\n",
    "    composite = alpha_pred[..., np.newaxis] * (image/255.0) + (1 - alpha_pred[..., np.newaxis]) * bg_color\n",
    "    composite = np.clip(composite, 0, 1)\n",
    "    axes[3].imshow(composite)\n",
    "    axes[3].set_title('Composite Result')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return alpha_pred\n",
    "\n",
    "print(\"‚úÖ Inference functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb84624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "best_checkpoint_path = os.path.join(config.checkpoint_dir, 'best_checkpoint.pth')\n",
    "if os.path.exists(best_checkpoint_path):\n",
    "    load_checkpoint(best_checkpoint_path)\n",
    "    \n",
    "    # Visualize predictions on validation set\n",
    "    print(\"üé® Visualizing predictions on validation samples...\")\n",
    "    visualize_predictions(num_samples=4)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No trained model found. Please run training first or load a pre-trained model.\")\n",
    "\n",
    "# Example: Run inference on custom image (uncomment and modify path)\n",
    "# custom_image_path = \"/path/to/your/image.jpg\"\n",
    "# if os.path.exists(custom_image_path):\n",
    "#     print(\"üîç Running inference on custom image...\")\n",
    "#     alpha_result = infer_on_custom_image(custom_image_path)\n",
    "# else:\n",
    "#     print(\"üìù To test on custom image, update custom_image_path variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a5bd5",
   "metadata": {},
   "source": [
    "## 10. Model Export and Deployment\n",
    "\n",
    "Export the trained model for deployment and create final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export trained model for deployment\n",
    "def export_model():\n",
    "    \"\"\"Export the trained model in different formats\"\"\"\n",
    "    \n",
    "    if not os.path.exists(best_checkpoint_path):\n",
    "        print(\"‚ùå No trained model found to export\")\n",
    "        return\n",
    "    \n",
    "    # Create export directory\n",
    "    export_dir = \"/content/exported_models\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the best model\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Export as TorchScript (for production deployment)\n",
    "    print(\"üì¶ Exporting TorchScript model...\")\n",
    "    dummy_input = (\n",
    "        torch.randn(1, 3, config.input_size, config.input_size).to(device),\n",
    "        torch.randn(1, 3, config.input_size, config.input_size).to(device),\n",
    "        torch.randn(1, 2, config.input_size, config.input_size).to(device)\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        traced_model = torch.jit.trace(model, dummy_input)\n",
    "        torchscript_path = os.path.join(export_dir, \"lite_matting_torchscript.pt\")\n",
    "        traced_model.save(torchscript_path)\n",
    "        print(f\"‚úÖ TorchScript model saved: {torchscript_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå TorchScript export failed: {e}\")\n",
    "    \n",
    "    # 2. Export model weights only\n",
    "    weights_path = os.path.join(export_dir, \"lite_matting_weights.pth\")\n",
    "    torch.save(model.state_dict(), weights_path)\n",
    "    print(f\"‚úÖ Model weights saved: {weights_path}\")\n",
    "    \n",
    "    # 3. Export complete model (architecture + weights)\n",
    "    full_model_path = os.path.join(export_dir, \"lite_matting_full.pth\")\n",
    "    torch.save(model, full_model_path)\n",
    "    print(f\"‚úÖ Full model saved: {full_model_path}\")\n",
    "    \n",
    "    # 4. Export ONNX model (for cross-platform deployment)\n",
    "    print(\"üì¶ Exporting ONNX model...\")\n",
    "    try:\n",
    "        onnx_path = os.path.join(export_dir, \"lite_matting.onnx\")\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=11,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['image', 'trimap', 'clicks'],\n",
    "            output_names=['alpha'],\n",
    "            dynamic_axes={\n",
    "                'image': {0: 'batch_size'},\n",
    "                'trimap': {0: 'batch_size'},\n",
    "                'clicks': {0: 'batch_size'},\n",
    "                'alpha': {0: 'batch_size'}\n",
    "            }\n",
    "        )\n",
    "        print(f\"‚úÖ ONNX model saved: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ONNX export failed: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ All models exported to: {export_dir}\")\n",
    "    print(\"üìã Export summary:\")\n",
    "    for file in os.listdir(export_dir):\n",
    "        file_path = os.path.join(export_dir, file)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"   - {file}: {size_mb:.1f} MB\")\n",
    "\n",
    "# Export the model\n",
    "export_model()\n",
    "\n",
    "# Download models to local machine\n",
    "print(\"\\nüíæ To download the trained models to your local machine:\")\n",
    "print(\"1. Navigate to the exported_models folder in Colab files\")\n",
    "print(\"2. Right-click and download each model file\")\n",
    "print(\"3. Or use the following code to zip and download:\")\n",
    "\n",
    "zip_code = '''\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Create a zip file with all models\n",
    "zip_path = \"/content/lite_matting_models.zip\"\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    for root, dirs, files in os.walk(\"/content/exported_models\"):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, file)\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download(zip_path)\n",
    "'''\n",
    "\n",
    "print(\"üìù Copy and run this code to download all models as a zip file:\")\n",
    "print(zip_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa62099c",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Summary\n",
    "\n",
    "You have successfully implemented and trained a LiteMatting model for high-quality image matting! Here's what we accomplished:\n",
    "\n",
    "‚úÖ **Model Architecture**: Implemented the complete MobileMatting network with:\n",
    "- Multi-Scale Local Pyramid Pooling Module (MSLPPM)\n",
    "- Global Feature Network Block (GFNB) \n",
    "- Efficient MobileNet-based backbone\n",
    "- Skip connections for feature fusion\n",
    "\n",
    "‚úÖ **Training Pipeline**: Built a comprehensive training system with:\n",
    "- Adobe Composition-1K dataset loading\n",
    "- Data augmentation and preprocessing\n",
    "- Combined loss functions (Alpha + Composition + Gradient + Laplacian)\n",
    "- Learning rate scheduling and optimization\n",
    "- Model checkpointing and tensorboard logging\n",
    "\n",
    "‚úÖ **Evaluation**: Implemented standard matting metrics:\n",
    "- Sum of Absolute Differences (SAD)\n",
    "- Mean Squared Error (MSE)\n",
    "- Gradient Error\n",
    "\n",
    "‚úÖ **Deployment**: Exported models in multiple formats:\n",
    "- PyTorch weights and full model\n",
    "- TorchScript for production\n",
    "- ONNX for cross-platform deployment\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Dataset Access**: Download the Adobe Composition-1K dataset using your student account\n",
    "2. **Training**: Run the training loop with your dataset\n",
    "3. **Hyperparameter Tuning**: Experiment with different learning rates, loss weights, and architectures\n",
    "4. **Production Deployment**: Use the exported models in your applications\n",
    "5. **Mobile Deployment**: The model is optimized for mobile devices\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Memory**: Reduce batch size if you encounter CUDA out of memory errors\n",
    "- **Speed**: Use mixed precision training with `torch.cuda.amp` for faster training\n",
    "- **Quality**: Experiment with different loss weights for your specific use case\n",
    "- **Data**: Add more diverse backgrounds and subjects to improve generalization\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Adobe Image Matting Dataset](https://sites.google.com/view/deepimagematting)\n",
    "- [Papers With Code - Image Matting](https://paperswithcode.com/task/image-matting)\n",
    "- [LFPNet Paper](https://arxiv.org/abs/2109.12252) (inspiration for this work)\n",
    "\n",
    "Happy matting! üé®‚ú®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
