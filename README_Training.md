# LiteMatting Training Project

A comprehensive implementation of the LiteMatting model for high-quality image matting (background removal) optimized for mobile deployment.

## ğŸš€ Features

- **Mobile-Optimized Architecture**: Efficient MobileNet-based backbone
- **State-of-the-Art Components**: MSLPPM, GFNB modules for superior performance
- **Comprehensive Training**: Multi-loss training with Adobe Composition-1K dataset
- **Easy Deployment**: Multiple export formats (PyTorch, TorchScript, ONNX)
- **Google Colab Ready**: Optimized for GPU training in Colab

## ğŸ“ Project Structure

```
LiteMatting/
â”œâ”€â”€ LiteMatting_Training.ipynb  # Main training notebook for Google Colab
â”œâ”€â”€ network.py                  # Model architecture
â”œâ”€â”€ dataset.py                  # Dataset loading and preprocessing
â”œâ”€â”€ losses.py                   # Loss functions and metrics
â”œâ”€â”€ train.py                    # Training script (alternative to notebook)
â”œâ”€â”€ inference.py                # Standalone inference script
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ test.py                     # Original test script
â””â”€â”€ README.md                   # This file
```

## ğŸ›  Setup Instructions

### For Google Colab (Recommended)

1. **Upload Project**: Upload this entire folder to Google Colab or your Google Drive

2. **Open Notebook**: Open `LiteMatting_Training.ipynb` in Google Colab

3. **Enable GPU**: Go to Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU

4. **Run All Cells**: Execute the notebook cells in order

### For Local Development

1. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Download Dataset**: Get Adobe Composition-1K dataset (requires academic access)

3. **Train Model**:
   ```bash
   python train.py
   ```

4. **Run Inference**:
   ```bash
   python inference.py --input image.jpg --output alpha.png
   ```

## ğŸ“Š Dataset

This project uses the **Adobe Composition-1K** dataset, the standard benchmark for image matting.

### Getting the Dataset

1. Visit: https://sites.google.com/view/deepimagematting
2. Request access with your academic email (student account)
3. Download and extract the dataset
4. Update the `DATASET_PATH` in the notebook

### Expected Directory Structure

```
Composition-1K/
â”œâ”€â”€ Training_set/
â”‚   â”œâ”€â”€ fg/           # Foreground images
â”‚   â”œâ”€â”€ alpha/        # Alpha mattes (ground truth)
â”‚   â”œâ”€â”€ bg/           # Background images
â”‚   â”œâ”€â”€ merged/       # Composite images
â”‚   â””â”€â”€ trimaps/      # Trimap annotations
â””â”€â”€ Test_set/
    â”œâ”€â”€ fg/
    â”œâ”€â”€ alpha/
    â”œâ”€â”€ bg/
    â”œâ”€â”€ merged/
    â””â”€â”€ trimaps/
```

## ğŸ— Model Architecture

The LiteMatting model consists of:

- **Backbone**: MobileNet-based encoder for efficiency
- **MSLPPM**: Multi-Scale Local Pyramid Pooling Module for multi-scale features
- **GFNB**: Global Feature Network Block for long-range context
- **Decoder**: Progressive upsampling with skip connections

### Key Features:
- **Parameters**: ~2.3M (mobile-friendly)
- **Input**: Image + Trimap + Click guidance
- **Output**: High-quality alpha matte
- **Resolution**: Supports up to 1024x1024 images

## ğŸ“ˆ Training Configuration

### Default Hyperparameters:
- **Input Size**: 512x512
- **Batch Size**: 4 (Colab) / 8 (Local with sufficient GPU memory)
- **Learning Rate**: 1e-4
- **Optimizer**: AdamW with cosine annealing
- **Epochs**: 50-100
- **Loss Weights**: Alpha=1.0, Composition=1.0, Gradient=1.0, Laplacian=1.0

### Loss Functions:
1. **Alpha Loss**: L1 loss in unknown regions
2. **Composition Loss**: L1 loss on composited images
3. **Gradient Loss**: Preserves fine details
4. **Laplacian Loss**: Ensures smoothness

## ğŸ“Š Evaluation Metrics

- **SAD (Sum of Absolute Differences)**: Lower is better
- **MSE (Mean Squared Error)**: Lower is better
- **Gradient Error**: Measures detail preservation

## ğŸš€ Inference

### Using the Jupyter Notebook:
Run the inference cells in `LiteMatting_Training.ipynb`

### Using the Standalone Script:
```bash
# Basic inference
python inference.py --input image.jpg --output alpha.png

# With custom trimap
python inference.py --input image.jpg --trimap trimap.png --output alpha.png

# Create composite with new background
python inference.py --input image.jpg --output alpha.png --composite result.jpg --background 255 0 0
```

### Trimap Options:
- `auto`: Automatic trimap generation
- `center`: Center-based trimap
- `path/to/trimap.png`: Custom trimap file

## ğŸ“¦ Model Export

The training notebook exports models in multiple formats:

1. **PyTorch Weights** (`.pth`): For PyTorch applications
2. **TorchScript** (`.pt`): For production deployment
3. **ONNX** (`.onnx`): For cross-platform deployment
4. **Full Model** (`.pth`): Complete model with architecture

## ğŸ¯ Performance Tips

### Memory Optimization:
- Reduce batch size if encountering CUDA OOM errors
- Use gradient checkpointing for larger models
- Enable mixed precision training with `torch.cuda.amp`

### Training Tips:
- Start with smaller input size (320x320) then increase
- Adjust loss weights based on your specific use case
- Use data augmentation to improve generalization
- Monitor validation metrics to prevent overfitting

### Inference Optimization:
- Use TorchScript for faster inference
- Consider model quantization for mobile deployment
- Batch multiple images for better GPU utilization

## ğŸ” Troubleshooting

### Common Issues:

1. **CUDA Out of Memory**:
   - Reduce batch size
   - Use smaller input resolution
   - Enable gradient checkpointing

2. **Dataset Not Found**:
   - Check `DATASET_PATH` variable
   - Ensure dataset structure matches expected format
   - Verify file permissions

3. **Poor Results**:
   - Check if model weights loaded correctly
   - Verify trimap quality
   - Ensure proper data preprocessing

4. **Slow Training**:
   - Enable GPU in Colab
   - Use larger batch size if memory allows
   - Consider reducing input resolution temporarily

## ğŸ“š References

- [LFPNet Paper](https://arxiv.org/abs/2109.12252) - Inspiration for this work
- [Adobe Image Matting Dataset](https://sites.google.com/view/deepimagematting)
- [Papers With Code - Image Matting](https://paperswithcode.com/task/image-matting)

## ğŸ“„ License

This project is for academic and research purposes. Please respect the Adobe Composition-1K dataset license terms.

## ğŸ¤ Contributing

Feel free to submit issues, feature requests, or pull requests to improve this implementation.

## ğŸ“ Support

If you encounter any issues:
1. Check the troubleshooting section
2. Review the notebook outputs for error messages
3. Ensure all dependencies are properly installed
4. Verify dataset path and structure

---

Happy matting! ğŸ¨âœ¨
